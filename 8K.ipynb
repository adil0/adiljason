{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unicodedata\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import datetime as dt\n",
    "import smtplib\n",
    "import feedparser\n",
    "import xml.etree.ElementTree as ET\n",
    "import time\n",
    "import io\n",
    "import math\n",
    "import json\n",
    "from pandas.tseries.offsets import BDay\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the ticker data and create a data frame\n",
    "ticker_url = 'https://www.sec.gov/include/ticker.txt'\n",
    "\n",
    "# request that new content, this will not be a JSON STRUCTURE!\n",
    "content = requests.get(ticker_url).content\n",
    "data = content.decode(\"utf-8\").split('\\n')\n",
    "\n",
    "ticker_list = []\n",
    "for item in data:\n",
    "    ticker_dict = {}\n",
    "    ticker_dict['ticker'] = item.split('\\t')[0]\n",
    "    ticker_dict['cik'] = item.split('\\t')[1]\n",
    "    ticker_list.append(ticker_dict)\n",
    "\n",
    "ticker_df = pd.DataFrame(ticker_list) \n",
    "ticker_df['cik'] = ticker_df['cik'].astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BASIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###get stock information(from yahoo finance)\n",
    "def get_price_vol_desc(ticker):\n",
    "    print(ticker)\n",
    "    d = {}\n",
    "    try:\n",
    "        if type(ticker)== float:\n",
    "            return d\n",
    "        elif ticker != '':\n",
    "            res = requests.get('http://finance.yahoo.com/q?s=' + ticker)\n",
    "            soup = BeautifulSoup(res.text, 'html.parser')\n",
    "            \n",
    "            # get the summary stats table\n",
    "            script = soup.find('script', text=re.compile('root\\.App\\.main'))\n",
    "            json_text = re.search(r'^\\s*root\\.App\\.main\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                          script.string, flags=re.MULTILINE).group(1)\n",
    "            data = json.loads(json_text)\n",
    "            \n",
    "            d['ticker']         = ticker\n",
    "            d['Price']          = float(data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['regularMarketPrice']['raw'])\n",
    "            d['Prev_Close']     = float(data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['regularMarketPreviousClose']['raw'])\n",
    "            d['Open']           = float(data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['regularMarketOpen']['raw'])\n",
    "            d['Volume']         = float(data['context']['dispatcher']['stores']['QuoteSummaryStore']['summaryDetail']['regularMarketVolume']['raw'])\n",
    "            d['Avg_Volume']     = float(data['context']['dispatcher']['stores']['QuoteSummaryStore']['summaryDetail']['averageVolume']['raw'])            \n",
    "            d['Mkt_Cap']        = data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['marketCap']['fmt']\n",
    "            d['description']    = data['context']['dispatcher']['stores']['QuoteSummaryStore']['summaryProfile']['longBusinessSummary']\n",
    "            d['SharesOut']      = float(data['context']['dispatcher']['stores']['QuoteSummaryStore']['defaultKeyStatistics']['sharesOutstanding']['raw'])\n",
    "            d['Return']         = round(100*(d['Price']/d['Prev_Close']-1),2)\n",
    "            d['Gap_Return']     = round(100*(d['Open']/d['Prev_Close']-1),2)\n",
    "            d['Day_Return']     = round(100*(d['Price']/d['Open']-1),2)\n",
    "            d['Vol_Surprise']   = round((d['Volume']/d['Avg_Volume']),2)\n",
    "            d['mktCapChng']     = round(d['SharesOut']*d['Price']*d['Return']/100) \n",
    "            return d\n",
    "    except:\n",
    "        return d\n",
    "\n",
    "### basic stock information   \n",
    "def get_description(ticker):\n",
    "    d = {}\n",
    "    try:\n",
    "        res = requests.get('http://finance.yahoo.com/q?s=' + ticker)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        script = soup.find('script', text=re.compile('root\\.App\\.main'))\n",
    "        json_text = re.search(r'^\\s*root\\.App\\.main\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                          script.string, flags=re.MULTILINE).group(1)\n",
    "        data = json.loads(json_text)\n",
    "        d['ticker']         = ticker\n",
    "        d['Mkt_Cap']        = data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['marketCap']['fmt']\n",
    "        d['description']    = data['context']['dispatcher']['stores']['QuoteSummaryStore']['summaryProfile']['longBusinessSummary']\n",
    "        return d\n",
    "    except:\n",
    "        return d\n",
    "\n",
    "### extract pre-market data from yahoo finance    \n",
    "def get_preMktData(ticker):\n",
    "    d = {}\n",
    "    try:\n",
    "        res = requests.get('http://finance.yahoo.com/q?s=' + ticker)\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "        script = soup.find('script', text=re.compile('root\\.App\\.main'))\n",
    "        json_text = re.search(r'^\\s*root\\.App\\.main\\s*=\\s*({.*?})\\s*;\\s*$',\n",
    "                          script.string, flags=re.MULTILINE).group(1)\n",
    "        data = json.loads(json_text)\n",
    "        d['ticker']         = ticker\n",
    "        d['preMktRt']       = data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['preMarketChangePercent']['fmt']\n",
    "        d['Mkt_Cap']        = data['context']['dispatcher']['stores']['QuoteSummaryStore']['price']['marketCap']['fmt']\n",
    "        d['description']    = data['context']['dispatcher']['stores']['QuoteSummaryStore']['summaryProfile']['longBusinessSummary']\n",
    "        return d\n",
    "    except:\n",
    "        return d\n",
    "\n",
    "### basic filing data\n",
    "def scrape_txt(link):\n",
    "    pdict = {}\n",
    "    response = requests.get(link)\n",
    "    # pass it through the parser, in this case let's just use lxml because the tags seem to follow xml.\n",
    "    soup = BeautifulSoup(response.content, 'lxml')\n",
    "               \n",
    "    sec_header_tag = soup.find('sec-header')\n",
    "    ## get the date time and company data from header\n",
    "    header_list = list(filter(None, str(sec_header_tag).replace('\\t','').split(\"\\n\")))\n",
    "    item_info_indices = []\n",
    "    for index, el in enumerate(header_list):\n",
    "        if 'CENTRAL INDEX KEY:' in el:\n",
    "            cik_index = index     \n",
    "        elif 'FILER:' in el:\n",
    "            sic_index = index + 4  \n",
    "                        \n",
    "    ## get the cik and industry classification data\n",
    "    pdict['cik']                 = header_list[cik_index].split(\":\")[1]\n",
    "    pdict['industry_class']      = header_list[sic_index].split(\":\")[1]    \n",
    "    \n",
    "    return pdict\n",
    "\n",
    "### parse each filing\n",
    "def edgar_feed(url):\n",
    "    more_data_to_parse = True\n",
    "    d = feedparser.parse(url)\n",
    "    last_filing_date = datetime.strptime(d.entries[99].updated.split('T')[0], '%Y-%m-%d').date()\n",
    "    # if the last filing in the batch is from y'day, we ll stop after this batch\n",
    "    if last_filing_date < (datetime.today() - BDay(1)).date(): \n",
    "        more_data_to_parse = False\n",
    "    for entry in range(0,99):\n",
    "        filing_time = datetime.strptime(d.entries[entry].updated.split('-04:00')[0].replace('T',' '), '%Y-%m-%d %H:%M:%S')\n",
    "        # date time filter in the current batch\n",
    "        if filing_time < datetime.combine((datetime.today() - BDay(1)).date(), dt.time(16, 0, 0)):\n",
    "            break\n",
    "        company_name = d.entries[entry].title\n",
    "        company_name = company_name.split('- ')\n",
    "        company_name = company_name[1].split(' (')\n",
    "        company_name = company_name[0]\n",
    "        company_name = company_name.replace('.', '')\n",
    "        company_name = company_name.replace(',', '')\n",
    "        if '&amp;' in company_name:\n",
    "            company_name = company_name.replace('&amp;', '&')\n",
    "        if not company_name in stocks_parsed:\n",
    "            stocks_parsed.append(company_name) \n",
    "            filing_det = d.entries[entry].summary.split('<br>')[1:]\n",
    "            filing_type_ids = []\n",
    "            filing_types = []\n",
    "            for i in range(len(filing_det)):\n",
    "                sp_list = filing_det[i].split(\":\")\n",
    "                filing_type_ids.append(sp_list[0])\n",
    "                filing_types.append(sp_list[1].replace('\\n','').strip())        \n",
    "\n",
    "            filing_type_ids_str = ';'.join(map(str, filing_type_ids))\n",
    "            filing_types_str    = ';'.join(map(str, filing_types))\n",
    "\n",
    "            # parse the filing to get the CIK, SIC\n",
    "            link = d.entries[entry].link.replace('-index.htm','.txt')\n",
    "            sdict = scrape_txt(link)\n",
    "\n",
    "            # store the final parsed data \n",
    "            parsed_dict = {}\n",
    "            parsed_dict['company_name']        = company_name\n",
    "            parsed_dict['filing_url']          = d.entries[entry].link\n",
    "            parsed_dict['filing_datetime']     = filing_time\n",
    "            parsed_dict['filing_type_id']      = filing_type_ids_str\n",
    "            parsed_dict['filing_type']         = filing_types_str\n",
    "            parsed_dict['cik']                 = sdict['cik']\n",
    "            parsed_dict['industry_class']      = sdict['industry_class']\n",
    "            parsed_dict['filing_id']           = d.entries[entry].id.split(\":\")[-1].split(\"=\")[-1]        \n",
    "            \n",
    "            # now check the conditions to append the data to the master list\n",
    "            if set(filing_type_ids).intersection(set(filing_types_filter)) and parsed_dict['industry_class'] in industry_class_filter:\n",
    "                print(company_name)\n",
    "                master_list.append(parsed_dict)\n",
    "        else:\n",
    "            pass\n",
    "    return more_data_to_parse     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DAILY RUN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stocks_parsed = [] \n",
    "master_list = []\n",
    "filing_types_filter = ['Item 8.01', 'Item 7.01', 'Item 2.02']\n",
    "industry_class_filter = ['PHARMACEUTICAL PREPARATIONS [2834]', 'SERVICES-COMMERCIAL PHYSICAL &amp; BIOLOGICAL RESEARCH [8731]',\n",
    "                         'BIOLOGICAL PRODUCTS (NO DIAGNOSTIC SUBSTANCES) [2836]', 'SURGICAL &amp; MEDICAL INSTRUMENTS &amp; APPARATUS [3841]',\n",
    "                         'MEDICINAL CHEMICALS & BOTANICAL PRODUCTS [2833]']\n",
    "\n",
    "start = 0 \n",
    "more_data_to_parse = True\n",
    "while more_data_to_parse:\n",
    "    url = 'https://www.sec.gov/cgi-bin/browse-edgar?action=getcurrent&CIK=&type=8-k&company=&dateb=&owner=include&start='+str(start)+'&count=100&output=atom'\n",
    "    more_data_to_parse = edgar_feed(url)\n",
    "    if more_data_to_parse:\n",
    "        start = start+100\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(master_list)\n",
    "df = df.drop_duplicates(subset='filing_id', keep='first')\n",
    "#df['filing_datetime'] = df['filing_datetime'].apply(lambda x: datetime.strptime(x, '%Y%m%dT%H%M%S'))\n",
    "df['cik'] = df['cik'].astype('int')\n",
    "df = df.merge(ticker_df, how='left')\n",
    "df = df.drop_duplicates(subset='filing_id', keep='first')\n",
    "df= df.sort_values(by=['filing_datetime'])\n",
    "df = df[['company_name', 'filing_datetime', 'ticker', 'filing_url', 'filing_type','industry_class',\n",
    "        'cik', 'filing_id']]    \n",
    "date= str(datetime.today().date()).replace(\"-\",\"\")\n",
    "pdate = str((datetime.today() - BDay(1)).date()).replace(\"-\",\"\")\n",
    "file_name = 'RT8K'+date+'.csv'\n",
    "pd_file_name = 'RT8K'+pdate+'.csv'\n",
    "\n",
    "if datetime.today().time() > dt.time(17, 30, 0):\n",
    "    # remove the data from y'day file to avoid duplicates\n",
    "    pd_df = pd.read_csv(pd_file_name)\n",
    "    cut_off = datetime.combine((datetime.today() - BDay(1)).date(), dt.time(16, 0, 0))\n",
    "    #update y'day's file\n",
    "    pdd_df= pd_df[pd_df.filing_datetime.apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))<cut_off]\n",
    "    pdd_df.to_csv(pd_file_name, index=False)\n",
    "    # update today's file\n",
    "    d = df.ticker.apply(get_price_vol_desc)\n",
    "    p_df = pd.DataFrame([d[1] for d in d.items()])\n",
    "    df = df.merge(p_df)\n",
    "    df = df[['company_name', 'filing_datetime', 'ticker', 'filing_url', 'filing_type','industry_class',\n",
    "             'cik', 'filing_id','Vol_Surprise', 'Return', 'Gap_Return', 'Day_Return','Mkt_Cap','mktCapChng',\n",
    "             'Price', 'Volume','Avg_Volume','Prev_Close','description']]        \n",
    "    df.drop_duplicates(subset='filing_id', keep='first', inplace=True)\n",
    "    df.to_csv(file_name, index=False)\n",
    "else:\n",
    "    d = df.ticker.apply(get_preMktData)\n",
    "    p_df = pd.DataFrame([d[1] for d in d.items()])\n",
    "    df = df.merge(p_df)\n",
    "    df = df[['company_name', 'filing_datetime', 'ticker', 'filing_url', 'filing_type','industry_class',\n",
    "             'cik', 'filing_id','preMktRt','Mkt_Cap','description']]  \n",
    "    df.drop_duplicates(subset='filing_id', keep='first', inplace=True)\n",
    "    df.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## save the data to csv file\n",
    "# remove the data from y'day file to avoid duplicates\n",
    "pd_df = pd.read_csv(pd_file_name)\n",
    "cut_off = datetime.combine((datetime.today() - BDay(2)).date(), dt.time(16, 0, 0))\n",
    "#update y'day's file\n",
    "pdd_df= pd_df[pd_df.filing_datetime.apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))<cut_off]\n",
    "pdd_df.to_csv(pd_file_name, index=False)\n",
    "# update today's file\n",
    "d = df.ticker.apply(get_price_vol_desc)\n",
    "p_df = pd.DataFrame([d[1] for d in d.items()])\n",
    "df = df.merge(p_df)\n",
    "df = df[['company_name', 'filing_datetime', 'ticker', 'filing_url', 'filing_type','industry_class',\n",
    "             'cik', 'filing_id','Vol_Surprise', 'Return', 'Gap_Return', 'Day_Return','Mkt_Cap','mktCapChng',\n",
    "             'Price', 'Volume','Avg_Volume','Prev_Close','description']]        \n",
    "df.drop_duplicates(subset='filing_id', keep='first', inplace=True)\n",
    "df.to_csv(file_name, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
